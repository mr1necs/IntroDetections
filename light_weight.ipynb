{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Подготовка окружения (импорт библиотек)",
   "id": "b956099519e94fb5"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-17T19:15:36.340027Z",
     "start_time": "2025-06-17T19:15:26.379565Z"
    }
   },
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.cluster import DBSCAN\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from tqdm.notebook import tqdm"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Конфигурация параметров",
   "id": "14b3e75ac849ece0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T19:16:15.480567Z",
     "start_time": "2025-06-17T19:16:15.477274Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Configuration\n",
    "BASE_DIR = Path().resolve()                         # Root directory of the project\n",
    "TRAIN_VIDEO_DIR = BASE_DIR / \"data_train_short/\"    # directory with train .mp4 files named as <video_id>.mp4\n",
    "TEST_VIDEO_DIR = BASE_DIR / \"data_test_short/\"      # directory with test  .mp4 files named as <video_id>.mp4\n",
    "TRAIN_JSON = BASE_DIR / \"train_labels.json\"         # json mapping train video_id -> {\"start\", \"end\"}\n",
    "TEST_JSON = BASE_DIR / \"test_labels.json\"           # json mapping test  video_id -> {\"start\", \"end\"}\n",
    "FRAME_RATE = 2                                      # fps for sampling\n",
    "WINDOW_SEC = 15                                     # window length in seconds\n",
    "STRIDE_SEC = 1                                      # stride in seconds\n",
    "EPS = 0.5                                           # DBSCAN eps for clustering\n",
    "MIN_SAMPLES = 5                                     # DBSCAN min_samples"
   ],
   "id": "a648e332ea13caa8",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T19:16:16.823085Z",
     "start_time": "2025-06-17T19:16:16.781862Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Device selection: MPS > CUDA > CPU\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "print(\"Model use device:\", DEVICE)"
   ],
   "id": "848ebbe5de69e852",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model use device: mps\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Инициализация модели и трансформаций",
   "id": "2c1f8936e06b71c6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T19:16:18.379676Z",
     "start_time": "2025-06-17T19:16:18.069366Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Select ResNet50 pretrained model\n",
    "torch.set_grad_enabled(False)                       # Disable gradients globally for inference\n",
    "model = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "model.fc = torch.nn.Identity()                      # Remove final classification layer\n",
    "model.to(DEVICE)\n",
    "model.eval()"
   ],
   "id": "dc8f5231270453ff",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Identity()\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T19:16:19.416961Z",
     "start_time": "2025-06-17T19:16:19.414070Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Image preprocessing pipeline\n",
    "transform = T.Compose([\n",
    "    T.ToPILImage(),\n",
    "    T.Resize((224, 224)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ],
   "id": "9fe2a35b65a5d246",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Извлечение кадров и признаков, подготовка окна",
   "id": "ad3e42a0e991e8af"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T19:16:20.450466Z",
     "start_time": "2025-06-17T19:16:20.447114Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Convert time string to seconds\n",
    "def time_to_seconds(t_str):\n",
    "    parts = list(map(int, t_str.split(':')))\n",
    "    if len(parts) == 3:\n",
    "        h, m, s = parts\n",
    "        return h * 3600 + m * 60 + s\n",
    "    elif len(parts) == 2:\n",
    "        m, s = parts\n",
    "        return m * 60 + s\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid time format: {t_str}\")\n"
   ],
   "id": "aed40f8c3113f8b",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T19:16:20.957442Z",
     "start_time": "2025-06-17T19:16:20.954353Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Extract frames at a given sampling rate\n",
    "def extract_frames(video_path: Path, rate: int = FRAME_RATE) -> list[np.ndarray]:\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS) or rate\n",
    "    step = max(int(fps / rate), 1)\n",
    "    frames = []\n",
    "    idx = 0\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        if idx % step == 0:\n",
    "            frames.append(frame)\n",
    "        idx += 1\n",
    "    cap.release()\n",
    "    return frames"
   ],
   "id": "c57bbd096768713",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T19:16:21.469040Z",
     "start_time": "2025-06-17T19:16:21.465675Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Extract feature vectors for a list of frames\n",
    "def extract_features(frames: list[np.ndarray]) -> np.ndarray:\n",
    "    feats = []\n",
    "    for img in frames:\n",
    "        tensor = transform(img).unsqueeze(0).to(DEVICE)\n",
    "        feature = model(tensor).cpu().numpy().ravel()\n",
    "        feats.append(feature)\n",
    "    return np.vstack(feats)"
   ],
   "id": "da63bf035a83bcf7",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T19:16:21.934105Z",
     "start_time": "2025-06-17T19:16:21.931079Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Generate sliding windows of feature vectors\n",
    "def sliding_windows(feats: np.ndarray, window_size: int, stride: int) -> list[tuple[int, np.ndarray]]:\n",
    "    n_frames = feats.shape[0]\n",
    "    return [\n",
    "        (start, feats[start:start + window_size].mean(axis=0))\n",
    "        for start in range(0, n_frames - window_size + 1, stride)\n",
    "    ]\n"
   ],
   "id": "5c725c1f97982696",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Построение шаблона на обучающей выборке",
   "id": "df042e274f7253ae"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T19:30:25.622434Z",
     "start_time": "2025-06-17T19:16:49.720855Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Build template from training data\n",
    "with open(TRAIN_JSON, 'r') as f:\n",
    "    train_labels = json.load(f)\n",
    "\n",
    "train_labels = dict(list(train_labels.items())[:15])\n",
    "\n",
    "all_feats, ids = [], []\n",
    "window_size = int(WINDOW_SEC * FRAME_RATE)\n",
    "stride = int(STRIDE_SEC * FRAME_RATE)\n",
    "\n",
    "for vid in tqdm(train_labels, desc=\"Building template\"):\n",
    "    video_path = TRAIN_VIDEO_DIR / vid / f\"{vid}.mp4\"\n",
    "    frames = extract_frames(video_path)\n",
    "    feats = extract_features(frames)\n",
    "    for start, vec in sliding_windows(feats, window_size, stride):\n",
    "        all_feats.append(vec)\n",
    "        ids.append(vid)\n",
    "\n",
    "all_feats = np.vstack(all_feats)\n",
    "labels = DBSCAN(eps=EPS, min_samples=MIN_SAMPLES).fit_predict(all_feats)\n",
    "\n",
    "# Select the largest cluster as template\n",
    "from collections import Counter\n",
    "counter = Counter(labels[labels >= 0])\n",
    "best_label = counter.most_common(1)[0][0]\n",
    "template = all_feats[labels == best_label].mean(axis=0)"
   ],
   "id": "fbaee032bedf26c1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Building template:   0%|          | 0/15 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "70f3fd4f17e64d259a3eb743abeadf22"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[h264 @ 0x329ce8710] Invalid NAL unit size (9848 > 6956).\n",
      "[h264 @ 0x329ce8710] missing picture in access unit with size 6960\n",
      "[h264 @ 0x329ce2d00] Invalid NAL unit size (9848 > 6956).\n",
      "[h264 @ 0x329ce2d00] Error splitting the input into NAL units.\n",
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x329c8eef0] stream 0, offset 0x12c00b4c: partial file\n",
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x329c8eef0] stream 1, offset 0x12c00c50: partial file\n",
      "[h264 @ 0x30a964e80] Invalid NAL unit size (7147 > 2825).\n",
      "[h264 @ 0x30a964e80] missing picture in access unit with size 2841\n",
      "[h264 @ 0x30a972550] Invalid NAL unit size (7147 > 2825).\n",
      "[h264 @ 0x30a972550] Error splitting the input into NAL units.\n",
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x33369d6f0] stream 0, offset 0x12c010e2: partial file\n",
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x33369d6f0] stream 0, offset 0x12c01984: partial file\n",
      "[h264 @ 0x30a965390] Invalid NAL unit size (7424 > 2860).\n",
      "[h264 @ 0x30a965390] missing picture in access unit with size 2876\n",
      "[h264 @ 0x30a994350] Invalid NAL unit size (7424 > 2860).\n",
      "[h264 @ 0x30a994350] Error splitting the input into NAL units.\n",
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x3093f36f0] stream 0, offset 0x12c011d4: partial file\n",
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x3093f36f0] stream 1, offset 0x12c018dc: partial file\n",
      "[h264 @ 0x30ab64b70] Invalid NAL unit size (10257 > 6247).\n",
      "[h264 @ 0x30ab64b70] missing picture in access unit with size 6263\n",
      "[h264 @ 0x30aba4f70] Invalid NAL unit size (10257 > 6247).\n",
      "[h264 @ 0x30aba4f70] Error splitting the input into NAL units.\n",
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x30a55d240] stream 1, offset 0x12c00faa: partial file\n",
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x30a55d240] stream 1, offset 0x12c0112d: partial file\n",
      "[h264 @ 0x3336b2c40] Invalid NAL unit size (1844 > 289).\n",
      "[h264 @ 0x3336b2c40] missing picture in access unit with size 305\n",
      "[h264 @ 0x3336cd2f0] Invalid NAL unit size (1844 > 289).\n",
      "[h264 @ 0x3336cd2f0] Error splitting the input into NAL units.\n",
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x30a55d240] stream 0, offset 0x12c00613: partial file\n",
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x30a55d240] stream 1, offset 0x12c047ee: partial file\n",
      "[h264 @ 0x309bff700] Invalid NAL unit size (13725 > 913).\n",
      "[h264 @ 0x309bff700] missing picture in access unit with size 917\n",
      "[h264 @ 0x341511960] Invalid NAL unit size (13725 > 913).\n",
      "[h264 @ 0x341511960] Error splitting the input into NAL units.\n",
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x174d767d0] stream 0, offset 0x12c0320c: partial file\n",
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x174d767d0] stream 1, offset 0x12c03c3e: partial file\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Делаем предсказания на test",
   "id": "1686139128eb8d8a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T19:35:33.256366Z",
     "start_time": "2025-06-17T19:31:38.940877Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Detect event segments in test videos\n",
    "with open(TEST_JSON, 'r') as f:\n",
    "    test_labels = json.load(f)\n",
    "\n",
    "test_labels = dict(list(test_labels.items())[:5])\n",
    "\n",
    "preds = {}\n",
    "for vid, info in tqdm(test_labels.items(), desc=\"Detecting events\"):\n",
    "    video_file = TEST_VIDEO_DIR / vid / f\"{vid}.mp4\"\n",
    "    frames = extract_frames(video_file)\n",
    "    feats = extract_features(frames)\n",
    "    best_start, best_dist = 0, float('inf')\n",
    "    for start, vec in sliding_windows(feats, window_size, stride):\n",
    "        dist = cosine(template, vec)\n",
    "        if dist < best_dist:\n",
    "            best_start, best_dist = start, dist\n",
    "    preds[vid] = {\n",
    "        'start': best_start / FRAME_RATE,\n",
    "        'end': (best_start + window_size) / FRAME_RATE,\n",
    "    }"
   ],
   "id": "724ff607c1494649",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Detecting events:   0%|          | 0/5 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b16044dce1b44e659d83dc5334db5047"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[h264 @ 0x30a572ba0] Invalid NAL unit size (8247 > 2791).\n",
      "[h264 @ 0x30a572ba0] missing picture in access unit with size 2795\n",
      "[h264 @ 0x30a5a65a0] Invalid NAL unit size (8247 > 2791).\n",
      "[h264 @ 0x30a5a65a0] Error splitting the input into NAL units.\n",
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x329ce11b0] stream 0, offset 0x12c01550: partial file\n",
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x329ce11b0] stream 0, offset 0x12c036be: partial file\n",
      "[h264 @ 0x330ec1b10] Invalid NAL unit size (14648 > 13963).\n",
      "[h264 @ 0x330ec1b10] missing picture in access unit with size 13967\n",
      "[h264 @ 0x30ab63840] Invalid NAL unit size (14648 > 13963).\n",
      "[h264 @ 0x30ab63840] Error splitting the input into NAL units.\n",
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x3093f2ca0] stream 0, offset 0x12c002ad: partial file\n",
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x3093f2ca0] stream 0, offset 0x12c038d8: partial file\n",
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x332f47520] stream 1, offset 0x12c00063: partial file\n",
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x332f47520] stream 1, offset 0x12c001ac: partial file\n",
      "[h264 @ 0x3336c0180] Invalid NAL unit size (9848 > 6956).\n",
      "[h264 @ 0x3336c0180] missing picture in access unit with size 6960\n",
      "[h264 @ 0x3336bb750] Invalid NAL unit size (9848 > 6956).\n",
      "[h264 @ 0x3336bb750] Error splitting the input into NAL units.\n",
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x174d41dd0] stream 0, offset 0x12c00b4c: partial file\n",
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x174d41dd0] stream 1, offset 0x12c00c50: partial file\n",
      "[h264 @ 0x30ab65fc0] Invalid NAL unit size (11068 > 10955).\n",
      "[h264 @ 0x30ab65fc0] missing picture in access unit with size 10959\n",
      "[h264 @ 0x30a996610] Invalid NAL unit size (11068 > 10955).\n",
      "[h264 @ 0x30a996610] Error splitting the input into NAL units.\n",
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x30a573b50] stream 0, offset 0x12c00071: partial file\n",
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x30a573b50] stream 0, offset 0x12c021f3: partial file\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Вычисление метрик качества",
   "id": "9b7daa320db3ecd7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T19:40:19.183114Z",
     "start_time": "2025-06-17T19:40:19.179292Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Evaluation metrics\n",
    "ios, start_err, end_err = [], [], []\n",
    "for vid, info in preds.items():\n",
    "    true_start = time_to_seconds(test_labels[vid]['start'])\n",
    "    true_end = time_to_seconds(test_labels[vid]['end'])\n",
    "    pred_start = preds[vid]['start']\n",
    "    pred_end = preds[vid]['end']\n",
    "\n",
    "    intersection = max(0, min(pred_end, true_end) - max(pred_start, true_start))\n",
    "    union = (pred_end - pred_start) + (true_end - true_start) - intersection\n",
    "    ios.append(intersection / union if union > 0 else 0)\n",
    "    start_err.append(abs(pred_start - true_start))\n",
    "    end_err.append(abs(pred_end - true_end))"
   ],
   "id": "5e5fd467f51b1c69",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T19:40:20.565397Z",
     "start_time": "2025-06-17T19:40:20.562675Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"Mean IoU:       {np.mean(ios):.3f}\")\n",
    "print(f\"Mean start err: {np.mean(start_err):.3f}s\")\n",
    "print(f\"Mean end err:   {np.mean(end_err):.3f}s\")"
   ],
   "id": "b7a8de3e751728a9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean IoU:       0.508\n",
      "Mean start err: 12.800s\n",
      "Mean end err:   12.200s\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b623151d762eb604"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
