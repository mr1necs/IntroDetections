{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Подготовка окружения (импорт библиотек)",
   "id": "a411c67f42ef1afc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.io as io\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision.models.video import r3d_18, R3D_18_Weights\n",
    "from tqdm.auto import tqdm"
   ],
   "id": "bb39c3409c47cf3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Конфигурация",
   "id": "ab62251f59b39710"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Configuration\n",
    "BASE_DIR = Path(__file__).parent                    # Root directory of the project\n",
    "TRAIN_VIDEO_DIR = BASE_DIR / \"data_train_short/\"    # directory with train .mp4 files named as <video_id>.mp4\n",
    "TEST_VIDEO_DIR = BASE_DIR / \"data_test_short/\"      # directory with test  .mp4 files named as <video_id>.mp4\n",
    "TRAIN_JSON = BASE_DIR / \"train_labels.json\"         # json mapping train video_id -> {\"start\", \"end\"}\n",
    "TEST_JSON = BASE_DIR / \"test_labels.json\"           # json mapping test  video_id -> {\"start\", \"end\"}\n",
    "FRAME_RATE = 2                                      # fps for sampling\n",
    "WINDOW_SEC = 15                                     # window length in seconds\n",
    "STRIDE_SEC = 1                                      # stride in seconds"
   ],
   "id": "a48944094854b539",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Device selection: MPS > CUDA > CPU\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "print(\"Model use device:\", DEVICE)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Инициализация модели и трансформаций",
   "id": "ba7639f811c3b6f6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Preprocessing transform for video clips\n",
    "video_transform = T.Compose([\n",
    "    T.Resize((112, 112)),  # resize frames\n",
    "    T.Normalize(mean=[0.43216, 0.394666, 0.37645], std=[0.22803, 0.22145, 0.216989]),\n",
    "])\n"
   ],
   "id": "18850cf19bad58b5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load pretrained 3D ResNet and adjust head\n",
    "torch.set_grad_enabled(False)\n",
    "model = r3d_18(weights=R3D_18_Weights.DEFAULT)\n",
    "model.fc = nn.Linear(model.fc.in_features, 2)  # binary output\n",
    "model.to(DEVICE)\n",
    "model.train()"
   ],
   "id": "c78734f7e5614d2c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Загрузка и слияние меток",
   "id": "6c6475f3b0599453"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load and merge label files\n",
    "def load_labels_from_dir(json_path: Path) -> dict:\n",
    "    with json_path.open('r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "train_labels = load_labels_from_dir(TRAIN_JSON)\n",
    "test_labels  = load_labels_from_dir(TEST_JSON)\n",
    "\n",
    "# Merge two dicts\n",
    "labels = {**train_labels, **test_labels}"
   ],
   "id": "599e57178bdc75a5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Convert time string to seconds\n",
    "def time_to_seconds(t_str):\n",
    "    parts = list(map(int, t_str.split(':')))\n",
    "    if len(parts) == 3:\n",
    "        h, m, s = parts\n",
    "        return h * 3600 + m * 60 + s\n",
    "    elif len(parts) == 2:\n",
    "        m, s = parts\n",
    "        return m * 60 + s\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid time format: {t_str}\")\n"
   ],
   "id": "773a89ed6651d5af",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Определение датасета",
   "id": "a2078768df430924"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Custom Dataset for intro detection\n",
    "class VideoIntroDataset(Dataset):\n",
    "    def __init__(self, labels: dict, video_dir: Path, transform=None):\n",
    "        self.labels = labels\n",
    "        self.video_dir = video_dir\n",
    "        self.video_ids = list(labels.keys())\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        vid = self.video_ids[idx]\n",
    "        path = self.video_dir / vid / f\"{vid}.mp4\"\n",
    "        video, _, _ = io.read_video(str(path), pts_unit='sec')\n",
    "        # optional: sample frames or clip\n",
    "        if self.transform:\n",
    "            video = self.transform(video)\n",
    "        label = torch.tensor(self.labels[vid]['label'], dtype=torch.long)\n",
    "        return video, label"
   ],
   "id": "f1c5c12ce93e862a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Обучение модели",
   "id": "2cb9a26066dc4975"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Training setup\n",
    "dataset = VideoIntroDataset(labels, TRAIN_VIDEO_DIR, transform=video_transform)\n",
    "dataloader = DataLoader(dataset, batch_size=15, shuffle=True)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()  # classification loss\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)"
   ],
   "id": "2fbeafb38ab9edf4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Training loop\n",
    "epochs = 5\n",
    "for epoch in range(1, epochs + 1):\n",
    "    total, correct = 0, 0\n",
    "    for videos, targets in tqdm(dataloader, desc=f\"Epoch {epoch}\"):\n",
    "        videos, targets = videos.to(DEVICE), targets.to(DEVICE)\n",
    "        logits = model(videos)\n",
    "        loss = criterion(logits, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        preds = logits.argmax(dim=1)\n",
    "        total += targets.size(0)\n",
    "        correct += (preds == targets).sum().item()\n",
    "\n",
    "    print(f\"Epoch {epoch} — Loss: {loss:.4f}, Acc: {correct/total:.3f}\")"
   ],
   "id": "25dbb467333ecbaf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Детекция и оценка на тестовых данных",
   "id": "a1e75ca6668b7fe0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Sliding-window intro detection\n",
    "def detect_intro(model, video_path: Path, clip_len=WINDOW_SEC, step=STRIDE_SEC, threshold=0.5) -> tuple[float, float]:\n",
    "    \"\"\"Returns (start_sec, end_sec) or (None, None) if no intro detected.\"\"\"\n",
    "    model.eval()\n",
    "    video, _, info = io.read_video(str(video_path), pts_unit=\"sec\")  # Video frames and metadata\n",
    "    fps = info['video_fps']\n",
    "    T_total = video.shape[0]\n",
    "    scores: list[float] = []\n",
    "    times: list[float] = []\n",
    "    # Slide over video\n",
    "    for t0 in range(0, max(1, T_total - clip_len + 1), step):\n",
    "        clip = video[t0:t0 + clip_len]  # select clip\n",
    "        # Prepare tensor: [T, C, H, W]\n",
    "        clip = clip.permute(0, 3, 1, 2) / 255.0\n",
    "        clip = torch.stack([video_transform(frame) for frame in clip])\n",
    "        clip = clip.unsqueeze(0).to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            logits = model(clip)\n",
    "            score = torch.sigmoid(logits).item()  # confidence\n",
    "        scores.append(score)\n",
    "        times.append(t0 / fps)\n",
    "    scores = np.array(scores)\n",
    "    times = np.array(times)\n",
    "    # Mask above threshold\n",
    "    mask = scores > threshold\n",
    "    if not mask.any():\n",
    "        return None, None\n",
    "    # Find longest continuous segment\n",
    "    max_len = 0\n",
    "    best = (0, 0)\n",
    "    start_i = None\n",
    "    for i, m in enumerate(mask):\n",
    "        if m and start_i is None:\n",
    "            start_i = i\n",
    "        if (not m or i == len(mask) - 1) and start_i is not None:\n",
    "            end_i = i if not m else i + 1\n",
    "            length = end_i - start_i\n",
    "            if length > max_len:\n",
    "                max_len = length\n",
    "                best = (start_i, end_i)\n",
    "            start_i = None\n",
    "    s, e = best\n",
    "    # Convert to seconds\n",
    "    start_sec = float(times[s])\n",
    "    end_sec = float(times[e - 1] + clip_len / fps)\n",
    "    return start_sec, end_sec"
   ],
   "id": "7856b7f23ab5db43",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# IoU for two intervals\n",
    "def iou_interval(a: int, b: int, c: int, d: int) -> float:\n",
    "    inter = max(0, min(b, d) - max(a, c))\n",
    "    union = (b - a) + (d - c) - inter\n",
    "    return inter / union if union > 0 else 0"
   ],
   "id": "fe06a46da5d38cc8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Inference and evaluation\n",
    "model.eval()\n",
    "ious = []\n",
    "for vid, info in tqdm(labels.items(), desc=\"Eval\"):\n",
    "    path = TEST_VIDEO_DIR / vid / f\"{vid}.mp4\"\n",
    "    # define detect_intro to return start/end in seconds\n",
    "    pred_s, pred_e = detect_intro(model, path)\n",
    "    if pred_s is None:\n",
    "        continue\n",
    "    gt_s = time_to_seconds(info['start'])\n",
    "    gt_e = time_to_seconds(info['end'])\n",
    "    ious.append(iou_interval(gt_s, gt_e, pred_s, pred_e))\n",
    "\n",
    "print(f\"Mean IoU: {sum(ious)/len(ious):.3f}\")"
   ],
   "id": "ab3bd971f9d75ba8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
